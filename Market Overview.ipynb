{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "you are using nologin method, data you access may be limited\n"
     ]
    }
   ],
   "source": [
    "from pynse import *\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "#import mplfinance as mpf\n",
    "import time, asyncio\n",
    "import matplotlib.dates as mdates\n",
    "import mplfinance as mpf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import mplfinance as mpf\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "import scipy.stats as stats\n",
    "from tvDatafeed import TvDatafeed,Interval\n",
    "tv=TvDatafeed()\n",
    "import concurrent.futures\n",
    "import concurrent.futures\n",
    "\n",
    "nse=Nse()\n",
    "#nse.bhavcopy(series=\"all\")\n",
    "#bhavcopy_full=nse.bhavcopy(series=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DELIV DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########LOADING INDEX########\n",
    "file_path = r\"C:\\Users\\99111\\OneDrive\\Desktop\\Stocklist.xlsx\"\n",
    "Index = pd.read_excel(file_path)\n",
    "Index = Index['All'].unique().tolist()\n",
    "\n",
    "#Price and symbol data\n",
    "folder_path = \"C:/Users/99111/OneDrive/Documents/pynse/Shashank work/data/raw/Delivdata/*.csv\"\n",
    "\n",
    "files = glob.glob(folder_path)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "df = df.rename(columns={'DATE1': 'Date'})\n",
    "#df = df['SYMBOL'].isin(Index)\n",
    "\n",
    "df['Open'] = np.where(df['OPEN_PRICE'] > 0, df['OPEN_PRICE'], df['OPEN'])\n",
    "df['High'] = np.where(df['HIGH_PRICE'] > 0, df['HIGH_PRICE'], df['HIGH'])\n",
    "df['Low'] = np.where(df['LOW_PRICE'] > 0, df['LOW_PRICE'], df['LOW'])\n",
    "df['Close'] = np.where(df['CLOSE_PRICE'] > 0, df['CLOSE_PRICE'], df['CLOSE'])\n",
    "df['Prevclose'] = np.where(df['PREV_CLOSE'] > 0, df['PREV_CLOSE'], df['PREVCLOSE']) \n",
    "\n",
    "df['TradedQuantity'] = np.where(df['TTL_TRD_QNTY'] > 0, df['TTL_TRD_QNTY'], df['TOTTRDQTY'])\n",
    "df['Numberoftrades'] = np.where(df['NO_OF_TRADES'] > 0, df['NO_OF_TRADES'], df['TOTALTRADES'])\n",
    "df['Averageprice'] = np.where(df['AVG_PRICE'] > 0, df['AVG_PRICE'], (df['Open']+df['High']+df['Low']+df['Close']+df['Prevclose'])/5)\n",
    "\n",
    "df['TURNOVER_LACS'] = df['TURNOVER_LACS'] / 100\n",
    "df['TOTTRDVAL'] = df['TOTTRDVAL'] / 10000000\n",
    "df['Turnover_CR'] = np.where(df['TURNOVER_LACS'] > 0, df['TURNOVER_LACS'], df['TOTTRDVAL'])\n",
    "\n",
    "df = df.loc[:, ['SYMBOL','Date', 'Prevclose', 'Open', 'High', 'Low', 'Close','Averageprice', 'TradedQuantity', 'Numberoftrades', 'Turnover_CR', 'DELIV_QTY', 'DELIV_PER']]\n",
    "df['uniquecode'] = df['SYMBOL'] + df['Date'].astype(str)\n",
    "df.drop_duplicates(subset='uniquecode', keep='first', inplace=True)\n",
    "df = df.sort_values(by='Date', ascending=True)\n",
    "df1 = df[df['SYMBOL'].isin(Index)]\n",
    "\n",
    "##################LOADING SECTORS AND MCAP########################\n",
    "\n",
    "#Data for PE and MCAP\n",
    "folder_path = \"C:/Users/99111/OneDrive/Desktop/Full data set/Price, PE, EBITDA , EV & Mcap/*.csv\"\n",
    "\n",
    "files = glob.glob(folder_path)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "PEMCAP = pd.concat(df_list)\n",
    "PEMCAP = PEMCAP[PEMCAP['CD_NSE Symbol'].isin(Index)]\n",
    "\n",
    "PEMCAP['NDP_Date'] = pd.to_datetime(PEMCAP['NDP_Date'])\n",
    "latestdate = PEMCAP['NDP_Date'].max()\n",
    "PEMCAP = PEMCAP[PEMCAP['NDP_Date'] >= latestdate]\n",
    "PEMCAP = PEMCAP.loc[:, ['CD_NSE Symbol', 'CD_Sector' , 'CD_Industry']].copy()\n",
    "PEMCAP = PEMCAP.rename(columns={'CD_NSE Symbol': 'SYMBOL'})\n",
    "\n",
    "#Combining data\n",
    "df2 = pd.merge(df1, PEMCAP, on='SYMBOL', how='left')\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "\n",
    "dates = df2['Date'].unique()\n",
    "dates = dates[-400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Industry = pd.DataFrame()\n",
    "Sectors = pd.DataFrame()\n",
    "Market = pd.DataFrame()\n",
    "\n",
    "up_sym = pd.DataFrame()\n",
    "down_sym = pd.DataFrame()\n",
    "\n",
    "\n",
    "for date in dates[:5]:\n",
    "    try:\n",
    "\n",
    "        df3 = df2[df2['Date'] == date]\n",
    "\n",
    "        df4 = df2[df2['Date'] < date]\n",
    "        latestdate = df4['Date'].max()\n",
    "        df4 = df4[df4['Date'] == latestdate]\n",
    "\n",
    "        df5 = pd.merge(df3, df4, on='SYMBOL', how='inner')\n",
    "        df5['Change'] = ((((df5['Close_x'] - df5['Prevclose_x']) / df5['Prevclose_x']))*100).round(2)\n",
    "        df5['up'] = np.where(df5['Change'] > 0, 1, 0)\n",
    "        df5['down'] = np.where(df5['Change'] < 0, 1, 0)\n",
    "        df5['Up_Percent'] = np.where(df5['up'] == 1, df5['Change'], 0)\n",
    "        df5['Down_Percent'] = np.where(df5['down'] == 1, df5['Change'], 0)\n",
    "        df5['count'] = 1\n",
    "\n",
    "        df6 = df5.groupby('CD_Industry_x')[['Turnover_CR_x','Prevclose_x', 'Close_x','count', 'Up_Percent', 'up', 'Down_Percent', 'down']].sum().reset_index().copy()\n",
    "        df6['Up_Change'] = (((df6['Up_Percent'] / df6['up']))*100).round(2)\n",
    "        df6['Down_Change'] = (((df6['Down_Percent'] / df6['down']))*100).round(2)\n",
    "        df6 = df6.fillna(0)\n",
    "        df6['change'] = ((df6['Up_Change'] + df6['Down_Change'])).round(2)\n",
    "        df6['Sum_Change'] = ((((df6['Close_x'] - df6['Prevclose_x']) / df6['Prevclose_x']))*10000).round(2)\n",
    "        MTO = df6['Turnover_CR_x'].sum()\n",
    "        df6['PercentTurnover'] = ((df6['Turnover_CR_x'] / MTO) * 100).round(2)\n",
    "        df6['Date'] = date\n",
    "        df6 = df6.loc[:, ['Date','CD_Industry_x', 'Turnover_CR_x', 'PercentTurnover','up', 'down', 'count', 'Up_Change', 'Down_Change','change','Sum_Change']]\n",
    "\n",
    "        df7 = df5.groupby('CD_Sector_x')[['Turnover_CR_x','Prevclose_x', 'Close_x','count', 'Up_Percent', 'up', 'Down_Percent', 'down']].sum().reset_index().copy()\n",
    "        df7['Up_Change'] = (((df7['Up_Percent'] / df7['up']))*100).round(2)\n",
    "        df7['Down_Change'] = (((df7['Down_Percent'] / df7['down']))*100).round(2)\n",
    "        df7 = df7.fillna(0)\n",
    "        df7['change'] = ((df7['Up_Change'] + df7['Down_Change'])).round(2)\n",
    "        df7['Sum_Change'] = ((((df7['Close_x'] - df7['Prevclose_x']) / df7['Prevclose_x']))*10000).round(2)\n",
    "        MTO = df7['Turnover_CR_x'].sum()\n",
    "        df7['PercentTurnover'] = ((df7['Turnover_CR_x'] / MTO) * 100).round(2)\n",
    "        df7['Date'] = date\n",
    "        df7 = df7.loc[:, ['Date','CD_Sector_x', 'Turnover_CR_x', 'PercentTurnover','up', 'down', 'count', 'Up_Change', 'Down_Change','change','Sum_Change']]\n",
    "        \n",
    "        df8 = df5.groupby('Date_x')[['Turnover_CR_x','Prevclose_x', 'Close_x','count', 'Up_Percent', 'up', 'Down_Percent', 'down']].sum().reset_index().copy()\n",
    "        df8['Up_Change'] = (((df8['Up_Percent'] / df8['up']))*100).round(2)\n",
    "        df8['Down_Change'] = (((df8['Down_Percent'] / df8['down']))*100).round(2)\n",
    "        df8 = df8.fillna(0)\n",
    "        df8['change'] = ((df8['Up_Change'] + df8['Down_Change'])).round(2)\n",
    "        df8['Sum_Change'] = ((((df8['Close_x'] - df8['Prevclose_x']) / df8['Prevclose_x']))*10000).round(2)\n",
    "\n",
    "        ups = df5[df5['up'] == 1]\n",
    "        ups = ups['SYMBOL'].unique().tolist()\n",
    "        ups = pd.DataFrame({str(date)[:10]: ups})\n",
    "\n",
    "        downs = df5[df5['down'] == 1]\n",
    "        downs = downs['SYMBOL'].unique().tolist()\n",
    "        downs = pd.DataFrame({str(date)[:10]: downs})\n",
    "\n",
    "        Market = pd.concat([Market, df8], axis=0, ignore_index=True)\n",
    "        Industry = pd.concat([Industry, df6], axis=0, ignore_index=True)\n",
    "        Sectors = pd.concat([Sectors, df7], axis=0, ignore_index=True)\n",
    "\n",
    "        up_sym = pd.concat([up_sym, ups], axis=1, ignore_index=False)\n",
    "        down_sym = pd.concat([down_sym, downs], axis=1, ignore_index=False)\n",
    "  \n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error occurred while creating chart for {date}: {str(e)}\")\n",
    "            continue  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "Industry = pd.DataFrame()\n",
    "Sectors = pd.DataFrame()\n",
    "Market = pd.DataFrame()\n",
    "\n",
    "Market_up_sym = pd.DataFrame()\n",
    "Market_down_sym = pd.DataFrame()\n",
    "\n",
    "Market_up_p = pd.DataFrame()\n",
    "Market_down_p = pd.DataFrame()\n",
    "\n",
    "def process_date(date):\n",
    "    try:\n",
    "        df3 = df2[df2['Date'] == date]\n",
    "\n",
    "        df4 = df2[df2['Date'] < date]\n",
    "        latestdate = df4['Date'].max()\n",
    "        df4 = df4[df4['Date'] == latestdate]\n",
    "\n",
    "        df5 = pd.merge(df3, df4, on='SYMBOL', how='inner')\n",
    "        df5['Change'] = ((((df5['Close_x'] - df5['Prevclose_x']) / df5['Prevclose_x']))*100).round(2)\n",
    "        df5['up'] = np.where(df5['Change'] > 0, 1, 0)\n",
    "        df5['down'] = np.where(df5['Change'] < 0, 1, 0)\n",
    "        df5['Up_Percent'] = np.where(df5['up'] == 1, df5['Change'], 0)\n",
    "        df5['Down_Percent'] = np.where(df5['down'] == 1, df5['Change'], 0)\n",
    "        df5['count'] = 1\n",
    "\n",
    "        df6 = df5.groupby('CD_Industry_x')[['Turnover_CR_x','Prevclose_x', 'Close_x','count', 'Up_Percent', 'up', 'Down_Percent', 'down']].sum().reset_index().copy()\n",
    "        df6['Up_Change'] = (((df6['Up_Percent'] / df6['up']))*100).round(2)\n",
    "        df6['Down_Change'] = (((df6['Down_Percent'] / df6['down']))*100).round(2)\n",
    "        df6 = df6.fillna(0)\n",
    "        df6['change'] = ((df6['Up_Change'] + df6['Down_Change'])).round(2)\n",
    "        df6['Sum_Change'] = ((((df6['Close_x'] - df6['Prevclose_x']) / df6['Prevclose_x']))*10000).round(2)\n",
    "        MTO = df6['Turnover_CR_x'].sum()\n",
    "        df6['PercentTurnover'] = ((df6['Turnover_CR_x'] / MTO) * 100).round(2)\n",
    "        df6['Date'] = date\n",
    "        df6 = df6.loc[:, ['Date','CD_Industry_x', 'Turnover_CR_x', 'PercentTurnover','up', 'down', 'count', 'Up_Change', 'Down_Change','change','Sum_Change']]\n",
    "\n",
    "        df7 = df5.groupby('CD_Sector_x')[['Turnover_CR_x','Prevclose_x', 'Close_x','count', 'Up_Percent', 'up', 'Down_Percent', 'down']].sum().reset_index().copy()\n",
    "        df7['Up_Change'] = (((df7['Up_Percent'] / df7['up']))*100).round(2)\n",
    "        df7['Down_Change'] = (((df7['Down_Percent'] / df7['down']))*100).round(2)\n",
    "        df7 = df7.fillna(0)\n",
    "        df7['change'] = ((df7['Up_Change'] + df7['Down_Change'])).round(2)\n",
    "        df7['Sum_Change'] = ((((df7['Close_x'] - df7['Prevclose_x']) / df7['Prevclose_x']))*10000).round(2)\n",
    "        MTO = df7['Turnover_CR_x'].sum()\n",
    "        df7['PercentTurnover'] = ((df7['Turnover_CR_x'] / MTO) * 100).round(2)\n",
    "        df7['Date'] = date\n",
    "        df7 = df7.loc[:, ['Date','CD_Sector_x', 'Turnover_CR_x', 'PercentTurnover','up', 'down', 'count', 'Up_Change', 'Down_Change','change','Sum_Change']]\n",
    "        \n",
    "        df8 = df5.groupby('Date_x')[['Turnover_CR_x','Prevclose_x', 'Close_x','count', 'Up_Percent', 'up', 'Down_Percent', 'down']].sum().reset_index().copy()\n",
    "        df8['Up_Change'] = (((df8['Up_Percent'] / df8['up']))*100).round(2)\n",
    "        df8['Down_Change'] = (((df8['Down_Percent'] / df8['down']))*100).round(2)\n",
    "        df8 = df8.fillna(0)\n",
    "        df8['change'] = ((df8['Up_Change'] + df8['Down_Change'])).round(2)\n",
    "        df8['Sum_Change'] = ((((df8['Close_x'] - df8['Prevclose_x']) / df8['Prevclose_x']))*10000).round(2)\n",
    "\n",
    "        ups = df5[df5['up'] == 1]\n",
    "        ups = ups['SYMBOL'].tolist()\n",
    "        ups = pd.DataFrame({str(date)[:10]: ups})\n",
    "\n",
    "        downs = df5[df5['down'] == 1]\n",
    "        downs = downs['SYMBOL'].tolist()\n",
    "        downs = pd.DataFrame({str(date)[:10]: downs})\n",
    "\n",
    "        #ups_p = df5[df5['up'] == 1]\n",
    "        #ups_p = ups_p['Change'].tolist()\n",
    "        #ups_p = pd.DataFrame({str(date)[:10]: ups_p})\n",
    "\n",
    "        #downs_p = df5[df5['down'] == 1]\n",
    "        #downs_p = downs_p['Change'].tolist()\n",
    "        #downs_p = pd.DataFrame({str(date)[:10]: downs_p})\n",
    "\n",
    "        return df6, df7, df8, ups , downs #, ups_p, downs_p\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing date {date}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Assuming 'dates' is a list of dates\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_date, dates))\n",
    "\n",
    "# Concatenate the non-empty results to the main dataframes\n",
    "for result in results:\n",
    "    if result[0] is not None:\n",
    "        Industry = pd.concat([Industry, result[0]], axis=0, ignore_index=True)\n",
    "    if result[1] is not None:\n",
    "        Sectors = pd.concat([Sectors, result[1]], axis=0, ignore_index=True)\n",
    "    if result[2] is not None:\n",
    "        Market = pd.concat([Market, result[2]], axis=0, ignore_index=True)\n",
    "    if result[3] is not None:\n",
    "        Market_up_sym = pd.concat([Market_up_sym, result[3]], axis=1, ignore_index=False)\n",
    "    if result[4] is not None:\n",
    "        Market_down_sym = pd.concat([Market_down_sym, result[4]], axis=1, ignore_index=False)\n",
    "    #if result[5] is not None:\n",
    "    #    Market_up_p = pd.concat([Market_up_p, result[5]], axis=1, ignore_index=False)\n",
    "    #if result[6] is not None:\n",
    "    #    Market_down_p = pd.concat([Market_down_p, result[6]], axis=1, ignore_index=False)\n",
    "\n",
    "Market_up_sym = Market_up_sym.transpose()\n",
    "Market_up_sym = Market_up_sym.reset_index(drop=False)\n",
    "Market_up_sym['index'] = pd.to_datetime(Market_up_sym['index'])\n",
    "\n",
    "Market_down_sym = Market_down_sym.transpose()\n",
    "Market_down_sym = Market_down_sym.reset_index(drop=False)\n",
    "Market_down_sym['index'] = pd.to_datetime(Market_down_sym['index'])\n",
    "\n",
    "#Market_up_p = Market_up_p.transpose()\n",
    "#Market_up_p = Market_up_p.reset_index(drop=False)\n",
    "#Market_up_p['index'] = pd.to_datetime(Market_up_p['index'])\n",
    "\n",
    "#Market_down_p = Market_down_p.transpose()\n",
    "#Market_down_p = Market_down_p.reset_index(drop=False)\n",
    "#Market_down_p['index'] = pd.to_datetime(Market_down_p['index'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sectors['sectorcodes'] = Sectors['CD_Sector_x'] + Sectors['Date'].astype(str)\n",
    "Industry['industrycodes'] = Industry['CD_Industry_x'] + Industry['Date'].astype(str)\n",
    "\n",
    "Secs = Sectors['CD_Sector_x'].unique().tolist()\n",
    "Inds = Industry['CD_Industry_x'].unique().tolist()\n",
    "\n",
    "ds = pd.DataFrame()  # Assuming ds is a DataFrame initialized before the loop\n",
    "di = pd.DataFrame()  # Assuming ds is a DataFrame initialized before the loop\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def process_sector(Sec):\n",
    "    try:\n",
    "        s = Sectors[Sectors['CD_Sector_x'] == Sec].copy()\n",
    "\n",
    "        s['TO_Flag'] = (s['Turnover_CR_x'] / s['Turnover_CR_x'].rolling(252).quantile(0.85)).round(2)\n",
    "        s['PTO_Flag'] = (s['PercentTurnover'] / s['PercentTurnover'].rolling(252).quantile(0.85)).round(2)\n",
    "\n",
    "        return s\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing sector {Sec}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Assuming Secs is a list of sectors\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_sector, Secs))\n",
    "\n",
    "# Concatenate the non-empty results to the main DataFrame\n",
    "ds = pd.concat([result for result in results if result is not None], axis=0, ignore_index=True)\n",
    "\n",
    "##########################################################\n",
    "\n",
    "def process_sector(ind):\n",
    "    try:\n",
    "        s = Industry[Industry['CD_Industry_x'] == ind].copy()\n",
    "\n",
    "        s['TO_Flag'] = (s['Turnover_CR_x'] / s['Turnover_CR_x'].rolling(252).quantile(0.85)).round(2)\n",
    "        s['PTO_Flag'] = (s['PercentTurnover'] / s['PercentTurnover'].rolling(252).quantile(0.85)).round(2)\n",
    "\n",
    "        return s\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing sector {ind}: {str(e)}\")\n",
    "        return \n",
    "\n",
    "# Assuming Inds is a list of sectors\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_sector, Inds))\n",
    "\n",
    "# Concatenate the non-empty results to the main DataFrame\n",
    "di = pd.concat([result for result in results if result is not None], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "####### Creating prices df###########\n",
    "\n",
    "symdates = df2['Date'].unique()\n",
    "symdates = symdates[-15:]\n",
    "symdates.min()\n",
    "\n",
    "df9 = df2[df2['Date'] >= symdates.min()].copy()\n",
    "\n",
    "df9['sector_codes'] = df9['CD_Sector'] + df9['Date'].astype(str)\n",
    "df9['industry_codes'] = df9['CD_Industry'] + df9['Date'].astype(str)\n",
    "df9['Change'] = ((((df9['Close'] - df9['Prevclose']) / df9['Prevclose']))*100).round(2)\n",
    "df10 = df9.loc[:, ['Date','SYMBOL', 'Change', 'uniquecode','CD_Sector','CD_Industry','sector_codes', 'industry_codes']]\n",
    "\n",
    "##########################################################\n",
    "\n",
    "Market = Market.sort_values(by='Date_x', ascending=False)\n",
    "di = di.sort_values(by=['Date', 'Sum_Change'], ascending=[False, False])\n",
    "ds = ds.sort_values(by=['Date', 'Sum_Change'], ascending=[False, False])\n",
    "df10 = df10.sort_values(by=['Date', 'Change'], ascending=[False, False])\n",
    "\n",
    "##########################################################\n",
    "\n",
    "file_path = r'C:\\Users\\99111\\OneDrive\\Desktop\\daily updates\\Market Overview\\Market_Industry_Sectors.xlsx'\n",
    "Market['Breath'] = (((Market['up'] / Market['count']))*100).round(2)\n",
    "\n",
    "with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "    # Write each DataFrame to a specific sheet\n",
    "    Market.to_excel(writer, sheet_name='Market', index=False)\n",
    "    di.to_excel(writer, sheet_name='Industry', index=False)\n",
    "    ds.to_excel(writer, sheet_name='Sectors', index=False)\n",
    "    df10.to_excel(writer, sheet_name='price change', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############# INDUSTRY CODES and SECTOR CODES #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symdates = df2['Date'].unique()\n",
    "symdates = symdates[-5:]\n",
    "symdates.min()\n",
    "\n",
    "df9 = df2[df2['Date'] >= symdates.min()].copy()\n",
    "\n",
    "df9['sector_codes'] = df9['CD_Sector'] + df9['Date'].astype(str)\n",
    "df9['industry_codes'] = df9['CD_Industry'] + df9['Date'].astype(str)\n",
    "\n",
    "sector_codes = df9['sector_codes'].unique().tolist()\n",
    "industry_codes = df9['industry_codes'].unique().tolist()\n",
    "\n",
    "#########################################################\n",
    "\n",
    "Ind_up_sym = pd.DataFrame()\n",
    "Ind_down_sym = pd.DataFrame()\n",
    "\n",
    "def process_date(industry_code):\n",
    "    try:\n",
    "        df10 = df9[df9['industry_codes'] == industry_code].copy()\n",
    "        df10['Change'] = ((((df10['Close'] - df10['Prevclose']) / df10['Prevclose']))*100).round(2)\n",
    "        df10['up'] = np.where(df10['Change'] > 0, 1, 0)\n",
    "        df10['down'] = np.where(df10['Change'] < 0, 1, 0)\n",
    "\n",
    "        ups = df10[df10['up'] == 1]\n",
    "        ups1 = ups['SYMBOL'].tolist()\n",
    "        #ups2 = ups['Change'].tolist()\n",
    "        ups = pd.DataFrame({industry_code: ups1 })\n",
    "\n",
    "        downs = df10[df10['down'] == 1]\n",
    "        downs1 = downs['SYMBOL'].tolist()\n",
    "        #downs2 = downs['Change'].tolist()\n",
    "        downs = pd.DataFrame({industry_code: downs1 })\n",
    "\n",
    "\n",
    "        return  ups , downs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing date {industry_code}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Assuming 'dates' is a list of dates\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_date, industry_codes))\n",
    "\n",
    "# Concatenate the non-empty results to the main dataframes\n",
    "for result in results:\n",
    "    if result[0] is not None:\n",
    "        Ind_up_sym = pd.concat([Ind_up_sym, result[0]], axis=1, ignore_index=False)\n",
    "    if result[1] is not None:\n",
    "        Ind_down_sym = pd.concat([Ind_down_sym, result[1]], axis=1, ignore_index=False)\n",
    "\n",
    "Ind_up_sym = Ind_up_sym.transpose()\n",
    "Ind_up_sym = Ind_up_sym.reset_index(drop=False)\n",
    "Ind_up_sym = Ind_up_sym.fillna(0)\n",
    "\n",
    "Ind_down_sym = Ind_down_sym.transpose()\n",
    "Ind_down_sym = Ind_down_sym.reset_index(drop=False)\n",
    "Ind_down_sym = Ind_down_sym.fillna(0)\n",
    "\n",
    "########################SECTORS####################################################\n",
    "\n",
    "Sec_up_sym = pd.DataFrame()\n",
    "Sec_down_sym = pd.DataFrame()\n",
    "\n",
    "def process_date(sector_code):\n",
    "    try:\n",
    "        df10 = df9[df9['sector_codes'] == sector_code].copy()\n",
    "        df10['Change'] = ((((df10['Close'] - df10['Prevclose']) / df10['Prevclose']))*100).round(2)\n",
    "        df10['up'] = np.where(df10['Change'] > 0, 1, 0)\n",
    "        df10['down'] = np.where(df10['Change'] < 0, 1, 0)\n",
    "\n",
    "        ups = df10[df10['up'] == 1]\n",
    "        ups1 = ups['SYMBOL'].tolist()\n",
    "        #ups2 = ups['Change'].tolist()\n",
    "        ups = pd.DataFrame({sector_code: ups1 })\n",
    "\n",
    "        downs = df10[df10['down'] == 1]\n",
    "        downs1 = downs['SYMBOL'].tolist()\n",
    "        #downs2 = downs['Change'].tolist()\n",
    "        downs = pd.DataFrame({sector_code: downs1 })\n",
    "\n",
    "\n",
    "        return  ups , downs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing date {sector_code}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Assuming 'dates' is a list of dates\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_date, sector_codes))\n",
    "\n",
    "# Concatenate the non-empty results to the main dataframes\n",
    "for result in results:\n",
    "    if result[0] is not None:\n",
    "        Sec_up_sym = pd.concat([Sec_up_sym, result[0]], axis=1, ignore_index=False)\n",
    "    if result[1] is not None:\n",
    "        Sec_down_sym = pd.concat([Sec_down_sym, result[1]], axis=1, ignore_index=False)\n",
    "\n",
    "Sec_up_sym = Sec_up_sym.transpose()\n",
    "Sec_up_sym = Sec_up_sym.reset_index(drop=False)\n",
    "Sec_up_sym = Sec_up_sym.fillna(0)\n",
    "\n",
    "Sec_down_sym = Sec_down_sym.transpose()\n",
    "Sec_down_sym = Sec_down_sym.reset_index(drop=False)\n",
    "Sec_down_sym = Sec_down_sym.fillna(0)\n",
    "\n",
    "####### Creating prices df###########\n",
    "df9['Change'] = ((((df9['Close'] - df9['Prevclose']) / df9['Prevclose']))*100).round(2)\n",
    "df10 = df9.loc[:, ['Date','SYMBOL', 'Change', 'uniquecode','sector_codes', 'industry_codes']]\n",
    "df10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############### SAVING FILES ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(r'C:\\Users\\99111\\OneDrive\\Desktop\\daily updates\\Market Overview\\Symbols.xlsx', engine='xlsxwriter') as writer:\n",
    "    Ind_up_sym.to_excel(writer, sheet_name='Ind_up_sym', index=False, header=True)\n",
    "    Ind_down_sym.to_excel(writer, sheet_name='Ind_down_sym', index=False, header=True)\n",
    "    \n",
    "    Sec_up_sym.to_excel(writer, sheet_name='Sec_up_sym', index=False, header=True)\n",
    "    Sec_down_sym.to_excel(writer, sheet_name='Sec_down_sym', index=False, header=True)\n",
    "\n",
    "    Market_up_sym.to_excel(writer, sheet_name='Market_up_sym', index=False, header=True)\n",
    "    Market_down_sym.to_excel(writer, sheet_name='Market_down_sym', index=False, header=True)\n",
    "\n",
    "    Market_up_p.to_excel(writer, sheet_name='Market_up_sym', index=False, header=True)\n",
    "    Market_down_p.to_excel(writer, sheet_name='Market_down_sym', index=False, header=True)\n",
    "\n",
    "    df10.to_excel(writer, sheet_name='pricechange', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27156bc63d14a65dfa14e1b73b9a41f3aeb50270a4e898aa5ea771669f0a0420"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
